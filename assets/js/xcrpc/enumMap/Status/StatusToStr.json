{
    "0" : "Success",
    "1" : "Operation not permitted",
    "2" : "No such file or directory",
    "3" : "No such process",
    "4" : "Interrupted system call",
    "5" : "I/O error",
    "6" : "No such device or address",
    "7" : "Argument list too long",
    "8" : "Exec format error",
    "9" : "Bad file number",
    "10" : "No child processes",
    "11" : "Try again",
    "12" : "Out of resources",
    "13" : "Permission denied",
    "14" : "Bad address",
    "15" : "Block device required",
    "16" : "Device or resource busy",
    "17" : "File exists",
    "18" : "End of file",
    "19" : "Cross-device link",
    "20" : "No such device",
    "21" : "Not a directory",
    "22" : "Is a directory",
    "23" : "Invalid argument",
    "24" : "File table overflow",
    "25" : "Too many open files",
    "26" : "Not a typewriter",
    "27" : "Text file busy",
    "28" : "File too large",
    "29" : "No space left on device",
    "30" : "Illegal seek",
    "31" : "Read-only file system",
    "32" : "Too many links",
    "33" : "Broken pipe",
    "34" : "Math argument out of domain of func",
    "35" : "Math result not representable",
    "36" : "Resource deadlock would occur",
    "37" : "File name too long",
    "38" : "No record locks available",
    "39" : "Function not implemented",
    "40" : "Directory not empty",
    "41" : "Too many symbolic links encountered",
    "42" : "No message of desired type",
    "43" : "Identifier removed",
    "44" : "Channel number out of range",
    "45" : "Level 2 not synchronized",
    "46" : "Level 3 halted",
    "47" : "Level 3 reset",
    "48" : "Link number out of range",
    "49" : "Protocol driver not attached",
    "50" : "No CSI structure available",
    "51" : "Level 2 halted",
    "52" : "Invalid exchange",
    "53" : "Invalid request descriptor",
    "54" : "Exchange full",
    "55" : "No anode",
    "56" : "Invalid request code",
    "57" : "Invalid slot",
    "58" : "Bad font file format",
    "59" : "Device not a stream",
    "60" : "No data available",
    "61" : "Timer expired",
    "62" : "Out of streams resources",
    "63" : "Machine is not on the network",
    "64" : "Package not installed",
    "65" : "Object is remote",
    "66" : "Link has been severed",
    "67" : "Advertise error",
    "68" : "Srmount error",
    "69" : "Communication error on send",
    "70" : "Protocol error",
    "71" : "Multihop attempted",
    "72" : "RFS specific error",
    "73" : "Not a data message",
    "74" : "Value too large for defined data type",
    "75" : "Name not unique on network",
    "76" : "File descriptor in bad state",
    "77" : "Remote address changed",
    "78" : "Can not access a needed shared library",
    "79" : "Accessing a corrupted shared library",
    "80" : ".lib section in a.out corrupted",
    "81" : "Attempting to link in too many shared libraries",
    "82" : "Cannot exec a shared library directly",
    "83" : "Illegal byte sequence",
    "84" : "Interrupted system call should be restarted",
    "85" : "Streams pipe error",
    "86" : "Too many users",
    "87" : "Socket operation on non-socket",
    "88" : "Destination address required",
    "89" : "Message too long",
    "90" : "Protocol wrong type for socket",
    "91" : "Protocol not available",
    "92" : "Protocol not supported",
    "93" : "Socket type not supported",
    "94" : "Operation not supported on transport endpoint",
    "95" : "Protocol family not supported",
    "96" : "Address family not supported by protocol",
    "97" : "Address already in use",
    "98" : "Cannot assign requested address",
    "99" : "Network is down",
    "100" : "Network is unreachable",
    "101" : "Network dropped connection because of reset",
    "102" : "Software caused connection abort",
    "103" : "Connection reset by peer",
    "104" : "No buffer space available",
    "105" : "Transport endpoint is already connected",
    "106" : "Transport endpoint is not connected",
    "107" : "Cannot send after transport endpoint shutdown",
    "108" : "Too many references: cannot splice",
    "109" : "Connection timed out",
    "110" : "Connection refused",
    "111" : "Host is down",
    "112" : "No route to host",
    "113" : "Operation already in progress",
    "114" : "Operation now in progress",
    "115" : "Stale NFS file handle",
    "116" : "Structure needs cleaning",
    "117" : "Not a XENIX named type file",
    "118" : "No XENIX semaphores available",
    "119" : "Is a named type file",
    "120" : "Remote I/O error",
    "121" : "Quota exceeded",
    "122" : "No medium found",
    "123" : "Wrong medium type",
    "124" : "Operation Canceled",
    "125" : "Required key not available",
    "126" : "Key has expired",
    "127" : "Key has been revoked",
    "128" : "Key was rejected by service",
    "129" : "Owner died",
    "130" : "State not recoverable",
    "131" : "Operation not possible due to RF-kill",
    "132" : "Memory page has hardware error",
    "133" : "Output truncated",
    "134" : "Not implemented",
    "135" : "Unknown error",
    "136" : "msgLib delete() failed",
    "137" : "thrCreate() failed",
    "138" : "Thread was aborted",
    "139" : "libConfig open() failed",
    "140" : "libConfig seek failed",
    "141" : "libConfig flash open() failed",
    "142" : "libConfig flash lseek failed",
    "143" : "libConfig configDelete() failed",
    "144" : "Incorrect params to UsrNodeMain",
    "145" : "Unicode strings are not supported by this function",
    "146" : "Invalid value for ai_flags field",
    "147" : "NAME or SERVICE is unknown",
    "148" : "Non-recoverable failure in name resolution",
    "149" : "SERVICE not supported for socket type",
    "150" : "No address associated with NAME",
    "151" : "Address family for NAME not supported",
    "152" : "Request not canceled",
    "153" : "All requests done",
    "154" : "IDN encoding failed",
    "155" : "Last page",
    "156" : "More data to follow. Not end of stream",
    "157" : "Command not found",
    "158" : "Error parsing command",
    "159" : "Sched queue length exceeded",
    "160" : "Failure in the message layer",
    "161" : "Out of messages",
    "162" : "Shutdown message",
    "163" : "No such node exists in cluster",
    "164" : "New table created",
    "165" : "No such result set",
    "166" : "Data format does not support appending fields",
    "167" : "Data format does not support removing fields",
    "168" : "Failed to parse data format value",
    "169" : "Record data format is corrupt",
    "170" : "Field does not exist within record",
    "171" : "Unknown field type",
    "172" : "Failed to find a record corresponding to the given record number",
    "173" : "Searched value was not found",
    "174" : "Invalid data format",
    "175" : "Context does not support dereferencing a remote Fatptr",
    "176" : "Values buffer is too small to store even a single field value",
    "177" : "Too many values discovered for a single field",
    "178" : "Field type is not supported in this format",
    "179" : "Maximum number of dictionary segments reached",
    "180" : "Bad record identifier",
    "181" : "System has exceeded the configured maximum number of records; try increasing Constants.DfMaxRecords",
    "182" : "Type mismatch during index creation",
    "183" : "Intended key has more than a single",
    "184" : "Dataset not found",
    "185" : "Loading of this dataset has already started",
    "186" : "URL length is too large",
    "187" : "URL is not valid",
    "188" : "Data source type does not support file creation",
    "189" : "Data source type does not support file deletion",
    "190" : "Data source type does not support file rename",
    "191" : "Data source type does not support writing",
    "192" : "Data source type does not support seeking",
    "193" : "Seek failed",
    "194" : "Data source type does not support directory creation",
    "195" : "Data source type does not support directory removal",
    "196" : "Loading of this dataset failed",
    "197" : "Dataset is in use",
    "198" : "Data source does not support specified data format type",
    "199" : "Failed to initialize the mysql client library",
    "200" : "Failed to connect to mysql server & database",
    "201" : "Failed to run query against mysql table",
    "202" : "Failed to connect to the specified Data Source Name",
    "203" : "Failed to cleanup an internal nested error",
    "204" : "ODBC based database connections must be created outside of Xcalar",
    "205" : "Failed to bind variable to ODBC parameter",
    "206" : "Failed to create ODBC table",
    "207" : "Failed to export record to ODBC table",
    "208" : "Export table already exists",
    "209" : "Export table does not exist",
    "210" : "A target was added while targets were being listed",
    "211" : "The requested target already exists",
    "212" : "Data source type does not support file attributes",
    "213" : "Could not determine uncompressed file size",
    "214" : "Failed to shrink memory allocation",
    "215" : "name already exists",
    "216" : "Table already exists",
    "217" : "Invalid command. Could not find matching quotes",
    "218" : "Failed to compute the range partition hash function",
    "219" : "Field name cannot be blank",
    "220" : "No data dictionary defined for format type",
    "221" : "Could not find BTree associated with table handle",
    "222" : "BTree key type does not match insert message key type",
    "223" : "BTree dataset identifier does not match insert message dataset identifier",
    "224" : "Command is still running",
    "225" : "Invalid result set ID",
    "226" : "Cannot set position to beyond result set size",
    "227" : "Table is in use right now and cannot be deleted",
    "228" : "One of the lines in the CLI is too long",
    "229" : "Encountered an error reading from file",
    "230" : "Invalid table name",
    "231" : "Namespace name is too long",
    "232" : "Unexpected end-of-file attempting to read from socket",
    "233" : "stats group ID is invalid",
    "234" : "stats group name is invalid",
    "235" : "Invalid handle",
    "236" : "Error communicating across thrift connection",
    "237" : "Malformed BTree. BTree has no root",
    "238" : "Could not find key in BTree",
    "239" : "Could not find key-value pair",
    "240" : "Malformed eval string",
    "241" : "Could not find function",
    "242" : "Wrong number of arguments passed to function",
    "243" : "The new field name is too long",
    "244" : "The field name you entered already exists",
    "245" : "Wrong number of operands provided to operator",
    "246" : "Operation requires 1 operand",
    "247" : "Operation is not supported on input type",
    "248" : "Divide by zero error",
    "249" : "Operation returned NaN",
    "250" : "Mixed type is not supported in this xdf",
    "251" : "Aggregate output has insufficient size to store the result",
    "252" : "KV not found in table",
    "253" : "Listen to: Pretty Vacant by Sex Pistols",
    "254" : "Xdb is vacant",
    "255" : "Xdb is loading data",
    "256" : "Stale XdbHandle, Xdb not found",
    "257" : "Xdb cursor is uninitialized",
    "258" : "Task(s) failed",
    "259" : "The query ID does not exist",
    "260" : "There is no query job associate with this ID",
    "261" : "The query job is currently running",
    "262" : "API Task Failed",
    "263" : "The source table is already indexed by the specified key",
    "264" : "Some variables are undefined during evaluation",
    "265" : "The destination key/value buffer was full",
    "266" : "The module is not initialized yet",
    "267" : "Maximum number of joined values exceeded",
    "268" : "Xdb key type is already set",
    "269" : "Join keys must be of the same data type",
    "270" : "Joins may only be performed on tables with the same DHT",
    "271" : "Failed",
    "272" : "FileName entered is illegal",
    "273" : "File contents are empty",
    "274" : "Eval string entered is too long",
    "275" : "Table has been deleted",
    "276" : "Cant open the file",
    "277" : "Query failed",
    "278" : "Batch Query needs to run in new session",
    "279" : "Failed to create a DAG node",
    "280" : "Failed to delete a DAG node",
    "281" : "Failed to rename a DAG node",
    "282" : "Failed to change the state of DAG node",
    "283" : "No such field found while running aggregate",
    "284" : "Local function requires argument",
    "285" : "Accumulator is not inited",
    "286" : "Return value of aggregate is not a scalar",
    "287" : "Maximum number of tables and datasets reached",
    "288" : "Namespace name is in use",
    "289" : "Bad name for Namespace",
    "290" : "Name not found in Namespace",
    "291" : "Could not find dag node",
    "292" : "Update operation not supported",
    "293" : "Message response size would exceed maximum message payload size",
    "294" : "The requested key was not found",
    "295" : "The requested key's value doesn't equal the provided value",
    "296" : "Could not get amount of memory consumed",
    "297" : "No valid status received!",
    "298" : "No such aggregate operator!",
    "299" : "timed out waiting for table key type to resolve",
    "300" : "Variable name in evalString too long",
    "301" : "DAG handle not found",
    "302" : "DAG name is invalid",
    "303" : "DAG name is too long",
    "304" : "DAG name already exists",
    "305" : "DAG is empty",
    "306" : "DAG is not empty",
    "307" : "No more DAG nodes available",
    "308" : "DAG handle is not available",
    "309" : "DAG Node is currently in use",
    "310" : "DAG Node is in error state",
    "311" : "Operation not supported on the target",
    "312" : "DAG node is not ready",
    "313" : "Fail to destroy DAG handle",
    "314" : "Dataset has been loaded",
    "315" : "Dataset is not ready",
    "316" : "Session does not exist",
    "317" : "Session already exists",
    "318" : "Target session was not inactive",
    "319" : "Session user name is invalid",
    "320" : "Session has an unrecoverable error",
    "321" : "This user's session is present on another node",
    "322" : "The delete operation is not permitted",
    "323" : "Failed to load user-defined module/application",
    "324" : "A module with the given name already exists",
    "325" : "The specified user-defined module/application was not found",
    "326" : "The given module contains no functions",
    "327" : "Module name is invalid",
    "328" : "Module type is invalid",
    "329" : "Module source is invalid",
    "330" : "Module source is too large",
    "331" : "Failed to load function",
    "332" : "The specified function was not found in the given module",
    "333" : "Scalar Function name exceeds allowed length",
    "334" : "Scalar Function has too many parameters",
    "335" : "User-defined function/application variable or parameter name exceeds allowed length",
    "336" : "Variable type not supported by user-defined function/application",
    "337" : "Persisted user-defined function/application is invalid",
    "338" : "Failed to convert value to python data type",
    "339" : "Failed to execute user-defined function/application",
    "340" : "Invalid argument passed to user-defined function/application",
    "341" : "Failed to delete user-defined function/application on all nodes",
    "342" : "Token name in evalString is too long",
    "343" : "No configuration file specified",
    "344" : "Could not resolve result set schema",
    "345" : "DHT name is empty",
    "346" : "Upper bound is less than lower bound",
    "347" : "Checksum validation failed while reading log entry",
    "348" : "DHT chosen doesn't preserve sorted order!",
    "349" : "Maximum log entry size exceeded",
    "350" : "Log entry header is corrupt",
    "351" : "Log format not as expected",
    "352" : "Unrecognized log version",
    "353" : "Invalid KvStore key character",
    "354" : "System DHTs may not be modified",
    "355" : "The requested KvStore doesn't exist",
    "356" : "The CPU does not support Intel SSE 4.2 instructions",
    "357" : "Illegal character in user-defined function/application name",
    "358" : "License data is invalid",
    "359" : "Error opening the file",
    "360" : "Error reading the file",
    "361" : "Error writing the file",
    "362" : "The public key needed to validate the Xcalar license has not been properly provided",
    "363" : "The public key needed to validate the Xcalar license has an error.",
    "364" : "The id of the public key needed to validate the Xcalar license is out of range.",
    "365" : "The Xcalar license has not been properly provided",
    "366" : "The Xcalar license has an error.",
    "367" : "The signature of the Xcalar license is invalid",
    "368" : "The signature of the Xcalar license failed Base32 unmapping",
    "369" : "The data in the Xcalar license failed Base32 decoding",
    "370" : "The checksum of the Xcalar license is invalid",
    "371" : "An unknown error occurred during decoding of the Xcalar license key",
    "372" : "Xcalar license is invalid for this product",
    "373" : "Xcalar license file is too big, or too small",
    "374" : "Xcalar license has expired",
    "375" : "Xcalar license version is too old. Please obtain an updated version.",
    "376" : "The Xcalar Cluster has more nodes than the license allows",
    "377" : "The log file was already closed",
    "378" : "The log handle is not valid",
    "379" : "The cluster is in the process of shutting down",
    "380" : "Chosen ordering is not supported",
    "381" : "Failed to connect to HDFS volume",
    "382" : "Failed to get HDFS directory listing",
    "383" : "Available area is too small to render graph",
    "384" : "Mismatch between parameter type and node type",
    "385" : "The Parameter is too long",
    "386" : "Scheduled time may not exceed one year",
    "387" : "Scheduled period may not exceed one year",
    "388" : "The selected API is not parameterizable",
    "389" : "Could not find query record associated with the query id",
    "390" : "Joins may only be performed on tables with the same ordering",
    "391" : "Invalid user cookie provided",
    "392" : "Too many scheduled tasks",
    "393" : "Row has not been completely demystified",
    "394" : "Input is too large",
    "395" : "Failed to parse Xcalar configuration file",
    "396" : "The node ID is invalid",
    "397" : "There are no local nodes in the confiruation file",
    "398" : "Data source type does not support fallocate",
    "399" : "No file extension",
    "400" : "Export target is not supported",
    "401" : "Invalid creation rule specified",
    "402" : "No columns specified for export",
    "403" : "Too many columns specified for export",
    "404" : "Specified column name too long",
    "405" : "Empty result set cannot be exported",
    "406" : "Export requires a known schema",
    "407" : "Export file already exists",
    "408" : "Export file doesn't exist",
    "409" : "Monitor port number is invalid",
    "410" : "Export file and directory both exist with the same base name",
    "411" : "Files were corrupted during export",
    "412" : "Export file requires a new file be created",
    "413" : "Specified max export file size too small",
    "414" : "Cannot export to a single file while specifying header should be separate",
    "415" : "Cannot export append with a separate header file",
    "416" : "Cannot export append to a single file while adding a header",
    "417" : "Invalid header type specified",
    "418" : "Invalid split type specified",
    "419" : "Specified max file size must be greater than 0",
    "420" : "File version is unsupported",
    "421" : "Detected a file corruption",
    "422" : "An invalid request was sent to Xcalar",
    "423" : "Error occurred when using libarchive",
    "424" : "Failed to initialize send socket",
    "425" : "Node was skipped due to a previous error",
    "426" : "Field value truncated during cast",
    "427" : "Cast operation failed",
    "428" : "Log buffer is not aligned",
    "429" : "String encoding not supported",
    "430" : "Messaging interface with other process has been closed",
    "431" : "Operation has finished",
    "432" : "Operation statistics are not avaiable",
    "433" : "Failed to parse batch dataflow file",
    "434" : "Too many columns specified for batch dataflow",
    "435" : "Successfully overwrote user-defined module/application",
    "436" : "Support bundle generation failed",
    "437" : "Message payload too large to fit within message",
    "438" : "No childnode is available to process the operation",
    "439" : "Child process terminated",
    "440" : "Number of pages in an Xdb slot exceeds max extent sg elements",
    "441" : "Could not find aggregate result",
    "442" : "Maximum row size was exceeded",
    "443" : "Maximum directory depth exceeded",
    "444" : "Failed to open subdirectory",
    "445" : "Invalid dataset name provided",
    "446" : "Max statistics group size was exceeded",
    "447" : "Duplicate user-defined field found",
    "448" : "Type conversion error",
    "449" : "Operation not supported in prod build",
    "450" : "Out of available fault injection module slots",
    "451" : "No such errorpoint module",
    "452" : "No such errorpoint",
    "453" : "All specified files empty",
    "454" : "Stats group name too long",
    "455" : "Stats name too long",
    "456" : "Max statistics number was exceeded",
    "457" : "Stats group is full",
    "458" : "No files matching name pattern",
    "459" : "Invalid fatptr prefix or field name",
    "460" : "Duplicated immediate name",
    "461" : "Duplicated fatptr prefix",
    "462" : "List files is not supported for this source type",
    "463" : "Load done already called on this xdb",
    "464" : "Skip records must be specified with a record delimiter",
    "465" : "Parent process has terminated",
    "466" : "Replay session failed",
    "467" : "stack size is less than 2MB",
    "468" : "Target does not exist",
    "469" : "ODBC based database connections must be removed outside Xcalar",
    "470" : "Functional test disabled in this build",
    "471" : "Too many functional tests requested",
    "472" : "Target log corrupted",
    "473" : "Failed to convert from Python object to Xcalar type",
    "474" : "HDFS does not support read/write files",
    "475" : "No tables left in sessionGraph",
    "476" : "The table is empty",
    "477" : "Input regular expression is invalid",
    "478" : "User-defined function/application not found",
    "479" : "Too many outstanding APIs, try again later",
    "480" : "The supplied user name is not within the allowed size range",
    "481" : "Failed to inject Python module",
    "482" : "Invalid initialization",
    "483" : "Failed to parse user defined file list",
    "484" : "Invalid load arguments",
    "485" : "All work has been done.",
    "486" : "This user-defined function/application already exists. Delete before adding",
    "487" : "Too few parameters were passed to a user-defined function/application",
    "488" : "The reference count of the operation is incorrect",
    "489" : "Application name is invalid",
    "490" : "Application host type is invalid",
    "491" : "Application is too large",
    "492" : "Transaction recovery context initialization error",
    "493" : "Transaction recovery context default status",
    "494" : "Transaction recovery context was not found",
    "495" : "Transaction recovery resource is not being tracked",
    "496" : "Transaction recovery context state does not allow the requested operation",
    "497" : "Invalid GVM action",
    "498" : "Global variable provided does not exists",
    "499" : "OutputSize was corrupted",
    "500" : "Dataset name already exists",
    "501" : "Dataset has already been deleted",
    "502" : "Batch dataflow does not exist",
    "503" : "Dht specified does not exist",
    "504" : "Table specified does not exist",
    "505" : "Too many parameters in batch dataflow",
    "506" : "This parameter cannot be changed online",
    "507" : "Error occurs during Operation",
    "508" : "Operation has been cancelled",
    "509" : "The query name does not exist",
    "510" : "Parent node does not exist",
    "511" : "Load Application does not exist",
    "512" : "Failed to parse load application output",
    "513" : "Fault injection point triggered",
    "514" : "Fault injection set in 2 phase commit",
    "515" : "Export Application does not exist",
    "516" : "Session user currently in use",
    "517" : "Out of resources (Xcalar Managed Memory)",
    "518" : "App flags invalid",
    "519" : "Query job is already running",
    "520" : "Unknown Message for two PC barrier",
    "521" : "Timed out during cluster startup / shutdown synchronization",
    "522" : "Configured limit on child processes has been reached",
    "523" : "Limit on files loaded has been reached",
    "524" : "Resource temporarily unavailable, try again later",
    "525" : "Cannot export append to a single file with adding a header",
    "526" : "Cannot call an aggregate function during filter/map",
    "527" : "Dag node has been dropped",
    "528" : "Xdb slot has active cursor",
    "529" : "Failed to decode protobuf message",
    "530" : "Failed to load application",
    "531" : "XPU App does not exist",
    "532" : "Path is not on shared storage",
    "533" : "Failed to encode protobuf message",
    "534" : "JSON error occurred",
    "535" : "Message stream not found",
    "536" : "Value too small for defined data type",
    "537" : "No space left in page cache",
    "538" : "SchedTask functionality not supported anymore",
    "539" : "Operation disallowed as object is marked for removal",
    "540" : "Failed to extract output from application",
    "541" : "Failed to extract error message from application",
    "542" : "Inconsistency found in name space internal table",
    "543" : "Stale name space handle",
    "544" : "Invalid use of uninitialized handle",
    "545" : "Durable version not supported",
    "546" : "Durable data written by build from unclean workspace",
    "547" : "Maximum field size was exceeded",
    "548" : "The query name already exists",
    "549" : "Scalar Function module currently in use",
    "550" : "Target currently in use",
    "551" : "Cannot inactivate session, operations outstanding",
    "552" : "The DHT already exists",
    "553" : "The DHT is currently in use",
    "554" : "Number of result sets exceeded table meta limit",
    "555" : "The batch dataflow already exists",
    "556" : "The batch dataflow is currently in use",
    "557" : "Failed to compress buffer",
    "558" : "Failed to decompress buffer",
    "559" : "The query name is invalid",
    "560" : "The query has already already deleted",
    "561" : "The query is currently in use",
    "562" : "XDB serialization error",
    "563" : "XDB deserialization error",
    "564" : "XDB unexpectedly resident",
    "565" : "XDB unexpectedly not resident",
    "566" : "Session already inactivated",
    "567" : "Session is inactive",
    "568" : "Session user already deleted",
    "569" : "Session user does not exist",
    "570" : "Insufficient privilege to shutdown cluster",
    "571" : "Nothing available to serialize",
    "572" : "App with the given name already exists",
    "573" : "App specified was not found",
    "574" : "App currently in use",
    "575" : "Invalid stream context received",
    "576" : "Invalid number of bytes sent in payload",
    "577" : "Encountered partial failure in stats stream, cleaning up",
    "578" : "Log level is too large",
    "579" : "Client performed a handshake we didn't understand",
    "580" : "Query's metadata is on another node. Please go to that node",
    "581" : "App instance start error",
    "582" : "Timed out waiting for api",
    "583" : "IP address is too long",
    "584" : "Support bundle was created but not sent to Xcalar",
    "585" : "Encountered invalid stream protocol, cleaning up",
    "586" : "Encountered partial failure in blob stream, cleaning up",
    "587" : "Encountered unknown format in /proc/meminfo",
    "588" : "API received has invalid signature",
    "589" : "API received has invalid length",
    "590" : "LMDB Key/Value store error",
    "591" : "No buffers to receive on destination XPU",
    "592" : "Invalid ordering for Join",
    "593" : "Dataset is already locked",
    "594" : "XCE node alive, reap timed out",
    "595" : "Attempt to buffer logs failed",
    "596" : "Failure to unbuffer logs is fatal",
    "597" : "Failure in updating periodic log flushing",
    "598" : "The specified log level is not valid",
    "599" : "There are no users of the dataset",
    "600" : "Error parsing JSON query",
    "601" : "Required XEM configuration parameters not present",
    "602" : "Out of dataset memory",
    "603" : "Table is empty",
    "604" : "User info inaccessible since user initialization still in progress",
    "605" : "Target session was not active",
    "606" : "User failed to read sessions",
    "607" : "Generic protobuf error",
    "608" : "Record errors are disallowed",
    "609" : "Cannot replace key field",
    "610" : "Serialization is disabled",
    "611" : "Number of fields exceeded limit",
    "612" : "Wrong number of operands provided to operator",
    "613" : "Missing XcalarOpCode column",
    "614" : "Missing XcalarRankOver column",
    "615" : "XcalarOpCode column contains an invalid value",
    "616" : "XcalarRankOver column contains an invalid value",
    "617" : "Runtime parameters are invalid",
    "618" : "Invalid publish table name",
    "619" : "Publish table name already exists",
    "620" : "An unlicensed feature is in use",
    "621" : "The private key needed to sign the Xcalar license has not been properly provided",
    "622" : "The private key needed to sign the Xcalar license has an error",
    "623" : "The password needed to sign the Xcalar license is missing",
    "624" : "The Xcalar license is empty",
    "625" : "The Xcalar license signature is empty",
    "626" : "License file is bigger than internal buffer",
    "627" : "The password used to verify the license is not correct",
    "628" : "A license value is out of range",
    "629" : "Zlib initialization for the license failed during decompress",
    "630" : "An error occurred while decompressing the license",
    "631" : "The uncompressed license is too large",
    "632" : "Zlib initialization for the license failed during compress",
    "633" : "The specified operation on the license is not supported",
    "634" : "The specified operation has been disabled, because a proper license has not been provided or has expired",
    "635" : "Invalid workbook version",
    "636" : "Publish table name not found",
    "637" : "Publish table update not found",
    "638" : "An upgrade using the Xcalar upgrade tool is required",
    "639" : "Scalar Functions are not supported as part of filter condition in cross joins",
    "640" : "Invalid name supplied for the batch dataflow",
    "641" : "Serialization disabled due to missing serialization path",
    "642" : "Token name in evalString is invalid",
    "643" : "Function does not support array input",
    "644" : "Failed to thick allocate Buffer Cache due to out of memory",
    "645" : "Deserialization encountered an unknown IDL SHA",
    "646" : "Deserialization encountered wrong IDL version",
    "647" : "Incomplete session list (failed to read some sessions)",
    "648" : "Table being activated is dependent on inactive table",
    "649" : "Publish table is restoring",
    "650" : "Update requires a self select",
    "651" : "Required session name is missing",
    "652" : "XPU connection aborted due to internal error, try again later",
    "653" : "Publish table permission denied, since update is being attempted by non-owner Session",
    "654" : "Publish table permission denied, since coalesce is being attempted by non-owner Session",
    "655" : "Publish table owner node does not match the expected node",
    "656" : "Denied reference to Namespace object",
    "657" : "Checksum not found",
    "658" : "Checksum mismatch",
    "659" : "Invalid parameter for setting Runtime configuration",
    "660" : "Setting Runtime configuration not supported",
    "661" : "Publish table Snapshot is already in progress",
    "662" : "Scalar Function API invoked on non-owner node",
    "663" : "Scalar Function source has unexpected contents",
    "664" : "Scalar Function update failure during persistent write",
    "665" : "Scalar Function path not recognized",
    "666" : "Dataset metadata doesn't exist",
    "667" : "Dataset is already unloaded",
    "668" : "Scalar Function module name must be full path name",
    "669" : "Cgroups APIs have been disabled",
    "670" : "Cgroups App already in progress",
    "671" : "Error in JSON during session serialization",
    "672" : "Possible inconsistency in session metadata",
    "673" : "Setting Runtime configuration already in progress",
    "674" : "Limit for select number of rows reached",
    "675" : "Legacy target is not found",
    "676" : "Decimal floating point (DFP/Numeric) error",
    "677" : "Cgroup App already in progress",
    "678" : "Invalid KvStore key",
    "679" : "Invalid KvStore value",
    "680" : "Complex types in Parquet not supported",
    "681" : "Error parsing parquet file",
    "682" : "Unsupported Decimal type in Parquet",
    "683" : "Unsupported Logical type in Parquet Byte Array",
    "684" : "KVStore Ref Count Leak detected",
    "685" : "Pinned and cannot be dropped",
    "686" : "Already pinned",
    "687" : "Not pinned to unpin it",
    "688" : "Defered Free of XDB page header",
    "689" : "Dag node has invalid number of parents",
    "690" : "Invalid table name",
    "691" : "Table name already exists",
    "692" : "Table name not found",
    "693" : "Map failure summary update failed due to schema",
    "694" : "Map failure summary update not supported yet for failures in multiple eval strings",
    "695" : "Scalar Function execution failed in map ICV mode",
    "696" : "Mismatch in column name",
    "697" : "Mismatch in column type",
    "698" : "Mismatch in keys",
    "699" : "Mismatch in key name",
    "700" : "Mismatch in key type",
    "701" : "DHT mismatch",
    "702" : "Disallowed Rankover column in schema",
    "703" : "Disallowed OpCode column in schema",
    "704" : "Disallowed BatchId column in schema",
    "705" : "Disallowed Fatptr in schema",
    "706" : "Invalid Delta table schema",
    "707" : "Invalid table keys",
    "708" : "Starting cluster is still in progress, Retry",
    "709" : "No Cgroup Controller Paths Set -- XCE_CHILDNODE_PATHS does not exist",
    "710" : "Cgroup Controller Path Too Long",
    "711" : "Mismatch in ordering",
    "712" : "Table is in used by IMD and being locked",
    "713" : "Cannot have more than one export to table operation",
    "714" : "Mismatch in column position",
    "715" : "XDB page refcount error",
    "716" : "Stats collection of dataflow is in progress",
    "717" : "Table Id not found",
    "718" : "Invalid fully qualified table name",
    "719" : "Scalar Function create/update succeeded but flush failed",
    "720" : "Table not marked global",
    "721" : "Failed to parse string field length",
    "722" : "Failed to parse bool field",
    "723" : "Failed to parse int32 field",
    "724" : "Failed to parse int64 field",
    "725" : "Failed to parse uint32 field",
    "726" : "Failed to parse uint64 field",
    "727" : "Failed to parse float32 field",
    "728" : "Failed to parse float64 field",
    "729" : "Failed to parse Timestamp field",
    "730" : "Failed to parse numeric field",
    "731" : "Failed to parse proto value field length",
    "732" : "Failed to parse proto value field",
    "733" : "Invalid field type",
    "734" : "System app is disabled",
    "735" : "Union keys must be of the same data type",
    "736" : "Union may only be performed on tables with the same DHT",
    "737" : "Application is still in running state",
    "738" : "Max columns found for schema",
    "739" : "Encountered an internal error - please contact Xcalar support",
    "740" : "Value returned by Scalar Function exceeds maximum Xcalar field size",
    "741" : "A Scalar Function must always return a non None value",
    "742" : "The return type annotation in your Scalar Function <> is an unsupported data type. Supported data types are: bytes, bool, int, float.",
    "743" : "Scalar Function with 'bool' return type returning non-'bool' value",
    "744" : "Scalar Function with 'bytes' return type failed due to conversion or resource failure while trying to return the result",
    "745" : "Scalar Function tried to return a value with unsupported type"
}

